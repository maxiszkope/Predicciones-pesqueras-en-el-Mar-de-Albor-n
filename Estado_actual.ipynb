{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar DataFrame completo de la serie histórica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjudicar los caladeros a los puertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame de cuota_pesquera.\n",
    "df_cuota_pesquera = pd.read_csv(r'ruta a cuota_pesquera.csv')\n",
    "\n",
    "# Definir el diccionario de mapeo Puerto -> Caladeros.\n",
    "puerto_caladeros_dict = {\n",
    "    'Adra': [43, 46, 48, 49, 50, 52, 53, 55, 58, 60, 66, 78, 95, 133],\n",
    "    'Almería': [29, 32, 33, 34, 35, 38, 66, 73, 75, 77, 78, 79, 81],\n",
    "    'Carboneras': [29],\n",
    "    'Garrucha': [1, 4, 16, 20, 23],\n",
    "    'La Atunara': [99, 103, 106, 107, 108, 110, 113, 114, 115, 118, 119, 121, 123, 124, 125, 126, 129],\n",
    "    'La Caleta de Vélez': [64, 65, 67, 69, 70, 72, 80, 82, 85, 88, 89, 96, 98],\n",
    "    'Marbella': [98, 99, 101, 102, 103],\n",
    "    'Motril': [51, 54, 57, 58, 59, 63, 71, 74, 76, 128, 133],\n",
    "    'Málaga': [40, 41, 42, 44, 45, 47, 56, 59, 61, 62, 68, 69, 116],\n",
    "    'Roquetas de Mar': [29, 35, 39, 83, 84, 133]\n",
    "}\n",
    "\n",
    "# Asignar los caladeros correspondientes a cada puerto en df_cuota_pesquera.\n",
    "df_cuota_pesquera['Caladeros'] = df_cuota_pesquera['Puerto'].map(puerto_caladeros_dict)\n",
    "\n",
    "df_cuota_pesquera.drop(['CaladeroID'], axis=1, inplace=True)\n",
    "\n",
    "# Guardar el resultado en un nuevo archivo CSV.\n",
    "df_cuota_pesquera.to_csv('estado_actual.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unir al DataFrame los datos de temperatura, pH y clorofila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando años: 100%|██████████| 11/11 [06:55<00:00, 37.73s/it]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(r'ruta a estado_actual.csv')\n",
    "df2 = pd.read_csv(r'ruta a resultados_prediccion_temperatura.csv')\n",
    "df3 = pd.read_csv(r'ruta a resultados_prediccion_clorofila_con_pH.csv')\n",
    "\n",
    "# Diccionario de los meses.\n",
    "meses = {\n",
    "    'Enero': 1, 'Febrero': 2, 'Marzo': 3, 'Abril': 4, 'Mayo': 5, 'Junio': 6,\n",
    "    'Julio': 7, 'Agosto': 8, 'Septiembre': 9, 'Octubre': 10, 'Noviembre': 11, 'Diciembre': 12\n",
    "}\n",
    "\n",
    "# Mapear los meses en df_estado_actual.\n",
    "df1['Mes'] = df1['Mes'].map(meses)\n",
    "\n",
    "# Itera sobre cada año desde 2000 hasta 2024 con tqdm.\n",
    "for año in tqdm(range(2000, 2025), desc=\"Procesando años\"):\n",
    "    # Filtra los datos para el año actual.\n",
    "    df1_año = df1[df1['Año'] == año]\n",
    "    df2_año = df2[df2['Año'] == año]\n",
    "    df3_año = df3[df3['Año'] == año]\n",
    "    \n",
    "    # Unir los DataFrames para cada año y mes.\n",
    "    df_merged = pd.merge(df1_año, df2_año, on=['Año', 'Mes'], how='outer')\n",
    "    df_final = pd.merge(df_merged, df3_año, on=['Año', 'Mes'], how='outer')\n",
    "    \n",
    "    # Guardar el DataFrame resultante en un archivo CSV.\n",
    "    nombre_archivo = f'datos_año_{año}.csv'\n",
    "    df_final.to_csv(nombre_archivo, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unificar en un solo CSV todos los años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta que contiene los archivos CSV.\n",
    "carpeta = r'ruta a datos_completos_por_año'\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada archivo CSV.\n",
    "dfs = []\n",
    "\n",
    "# Obtener la cantidad total de archivos en la carpeta.\n",
    "total_archivos = len([archivo for archivo in os.listdir(carpeta) if archivo.endswith('.csv')])\n",
    "\n",
    "# Iterar sobre cada archivo en la carpeta con tqdm.\n",
    "for archivo in tqdm(os.listdir(carpeta), desc=\"Uniendo archivos\", total=total_archivos):\n",
    "    if archivo.endswith('.csv'):\n",
    "        # Leer el archivo CSV y añadirlo a la lista.\n",
    "        df = pd.read_csv(os.path.join(carpeta, archivo))\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo.\n",
    "df_evolucion_historica = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Guardar el DataFrame resultante en un archivo CSV.\n",
    "df_evolucion_historica.to_csv(r'D:\\BOOTCAMP\\datos_completos_por_año\\evolucion_historica.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que los recursos de Hardware del ordenador no permitan la ejecución debido a límites de memoria, esta acción se puede realizar mediante un pipeline en la plataforma Azure de Microsoft, debiendo de crear y activar una maquina virtual de almenos 32 GB de RAM debido al gran tamaño de los archivos.\n",
    "\n",
    "Con el archivo finalizado, se puede descargar el archivo CSV y descargar el archivo en la memoria local."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
